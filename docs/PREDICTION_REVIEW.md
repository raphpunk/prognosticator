# Prediction Review System

## Overview

The Prediction Review system provides comprehensive analysis and feedback tracking for all forecasts generated by the multi-agent system. This enables adaptive learning by recording outcomes and adjusting agent weights based on historical accuracy.

## Architecture

### Components

1. **prediction_report.py** (527 lines)
   - `AgentResponse`: Individual agent metrics (confidence, weights, execution time, tokens)
   - `DomainAnalysis`: Question classification with keyword extraction
   - `ConsensusAnalysis`: Agreement measurement and outlier detection
   - `PredictionMetadata`: Execution tracking (timing, cache hits, failures)
   - `PredictionReport`: Complete analysis package with quality scoring

2. **agents.py Integration**
   - Automatically generates reports after each prediction
   - Classifies domain using `DomainClassifier`
   - Records predictions in performance tracker database
   - Saves reports as JSON to `data/prediction_reports/`

3. **Prediction Review Tab** (scripts/app.py)
   - Browse historical predictions with filters
   - View detailed agent analysis
   - Mark outcomes (correct/incorrect/partial)
   - Analyze consensus patterns

## Data Flow

```
User Question → run_conversation()
    ↓
Multi-Agent Analysis
    ↓
DomainClassifier.classify()
    ↓
PredictionReportGenerator.generate_report()
    ↓
PerformanceTracker.record_prediction()
    ↓
Save JSON to data/prediction_reports/{prediction_id}.json
    ↓
Return prediction_id to UI
```

## Report Structure

Each prediction report contains:

### 1. Question & Domain Analysis
- Original question text
- Primary domain classification (10 domains)
- Domain confidence score
- Secondary domains
- Extracted keywords

### 2. Agent Responses
For each agent:
- Agent name and response text
- Base confidence level
- Base weight (from agent profile)
- Relevance boost (from domain matching)
- Performance boost (from historical accuracy)
- Final adjusted weight
- Execution time (ms)
- Model used
- Cache hit status
- Token counts (prompt/completion)

### 3. Consensus Analysis
- **Agreement Level**: unanimous/strong/moderate/weak/divergent
- **Consensus Strength**: 0.0-1.0 score
- **Outlier Agents**: Agents with significantly different views
- **Confidence Distribution**: Statistical breakdown

### 4. Quality Metrics
- **Data Quality Score** (0-1): Weighted composite of:
  - Consensus strength (30%)
  - Average confidence (25%)
  - Agent success rate (25%)
  - Performance factor (20%)

### 5. Key Insights
- Extracted important findings
- Common themes across agents
- Notable disagreements

### 6. Uncertainty Factors
- Low consensus warnings
- Data limitations
- Conflicting evidence

### 7. Execution Metadata
- Prediction ID (SHA-256 hash)
- Timestamp
- Total execution time
- Agent counts (total/succeeded/failed)
- Cache statistics
- Circuit breaker trips

## Using the Review Tab

### Filtering & Sorting

**Filter by Domain:**
- All, military, financial, energy, technology, climate, geopolitical, health, infrastructure, societal, policy

**Sort Options:**
- Recent First (default)
- Oldest First
- Highest Quality
- Lowest Quality

**Display Limit:** 5-100 reports

### Viewing Reports

1. **Report Selector**: Dropdown showing [quality score] timestamp - question
2. **Overview Metrics**: Quality, consensus, strength, agent counts
3. **Question & Domain**: Classification with confidence
4. **Outcome Marking**: Mark predictions as correct/incorrect/partial
5. **Agent Responses Table**: Sortable table with all weight components
6. **Full Responses**: Expandable section with complete agent text
7. **Consensus Analysis**: Agreement level and outlier detection
8. **Key Insights**: Important findings
9. **Uncertainty Factors**: Risk warnings
10. **Raw Data**: Full JSON export

### Marking Outcomes

Click one of three buttons:
- ✅ **Mark Correct**: Prediction was accurate
- ❌ **Mark Incorrect**: Prediction was wrong
- ⚠️ **Mark Partial**: Prediction was partially correct

Outcomes are recorded in `agent_performance.db` and used to:
1. Calculate historical accuracy per agent
2. Adjust performance boosts in future predictions
3. Identify domain specialists
4. Track system-wide accuracy trends

## Adaptive Learning Loop

```
Prediction Made
    ↓
Report Generated & Saved
    ↓
User Reviews Prediction
    ↓
User Marks Outcome (correct/incorrect/partial)
    ↓
PerformanceTracker.record_outcome()
    ↓
Update agent_metrics_cache
    ↓
Future Predictions Use Updated Performance Boosts
    ↓
Agents with Better Track Records Get Higher Weights
```

## Database Schema

### predictions table
```sql
CREATE TABLE predictions (
    prediction_id TEXT PRIMARY KEY,
    question TEXT NOT NULL,
    question_hash TEXT,
    domain TEXT,
    timestamp INTEGER
);
```

### agent_responses table
```sql
CREATE TABLE agent_responses (
    id INTEGER PRIMARY KEY,
    prediction_id TEXT,
    agent_name TEXT,
    response TEXT,
    confidence REAL,
    base_weight REAL,
    adjusted_weight REAL,
    FOREIGN KEY (prediction_id) REFERENCES predictions(prediction_id)
);
```

### prediction_outcomes table
```sql
CREATE TABLE prediction_outcomes (
    prediction_id TEXT PRIMARY KEY,
    outcome TEXT CHECK(outcome IN ('correct', 'incorrect', 'partial')),
    recorded_at INTEGER,
    notes TEXT,
    FOREIGN KEY (prediction_id) REFERENCES predictions(prediction_id)
);
```

### agent_metrics_cache table
```sql
CREATE TABLE agent_metrics_cache (
    agent_name TEXT PRIMARY KEY,
    accuracy REAL,
    total_predictions INTEGER,
    correct_predictions INTEGER,
    domain_performance TEXT,
    last_updated INTEGER
);
```

## File Locations

- **Reports**: `data/prediction_reports/{prediction_id}.json`
- **Database**: `data/agent_performance.db`
- **Module**: `src/forecasting/prediction_report.py`
- **UI**: `scripts/app.py` - "Prediction Review" tab

## Example Report

```json
{
  "question": "How likely is a major disruption to global semiconductor supply within 3 months?",
  "domain_analysis": {
    "primary_domain": "technology",
    "confidence": 0.85,
    "secondary_domains": ["geopolitical", "industrial"],
    "keywords_found": ["semiconductor", "supply", "global"]
  },
  "agent_responses": [
    {
      "agent_name": "Technology & Cyber Expert",
      "confidence": 0.65,
      "base_weight": 1.0,
      "relevance_boost": 1.5,
      "performance_boost": 1.2,
      "adjusted_weight": 1.8,
      "execution_time_ms": 1250,
      "model": "llama3.1:70b",
      "cached": false
    }
  ],
  "consensus_analysis": {
    "agreement_level": "strong",
    "consensus_strength": 0.78,
    "outlier_agents": [],
    "confidence_distribution": {
      "mean": 0.62,
      "std": 0.12,
      "min": 0.45,
      "max": 0.75
    }
  },
  "data_quality_score": 0.72,
  "key_insights": [
    "Strong consensus on moderate likelihood",
    "Technology experts show higher confidence",
    "Geopolitical factors identified as key risk"
  ],
  "uncertainty_factors": [
    "Limited recent data on supply chain status",
    "Divergent views on China-Taiwan tensions"
  ],
  "metadata": {
    "prediction_id": "abc123...",
    "timestamp": "2024-01-15T10:30:00Z",
    "total_execution_time_ms": 15000,
    "total_agents": 16,
    "successful_agents": 14,
    "failed_agents": 2,
    "cache_hits": 3,
    "circuit_breaker_trips": 0
  }
}
```

## Future Enhancements

### Planned Features
- [ ] Bayesian confidence calibration
- [ ] Agent accuracy trends visualization
- [ ] Domain-specific performance charts
- [ ] Automated outcome detection (web scraping)
- [ ] Ensemble method comparison
- [ ] Real-time performance dashboard
- [ ] Export reports to CSV/PDF
- [ ] Bulk outcome marking
- [ ] Time-series accuracy analysis
- [ ] A/B testing different consensus algorithms

### Performance Optimization
- [ ] Report pagination for large datasets
- [ ] Lazy loading of agent responses
- [ ] Database indexing on domain/timestamp
- [ ] Report caching layer
- [ ] Compressed JSON storage

## Best Practices

### For Analysts
1. **Review predictions systematically**: Don't cherry-pick, review chronologically
2. **Mark outcomes promptly**: Fresher memory = better accuracy assessment
3. **Use partial outcomes**: Many predictions are neither fully right nor wrong
4. **Add outcome notes**: Context helps improve the system
5. **Monitor quality scores**: Low scores indicate unreliable predictions

### For System Operators
1. **Regular database maintenance**: Vacuum SQLite databases monthly
2. **Archive old reports**: Move reports >1 year to cold storage
3. **Monitor disk usage**: JSON reports can accumulate
4. **Track cache hit rates**: Optimize for frequently asked patterns
5. **Review circuit breaker logs**: Identify Ollama reliability issues

## Troubleshooting

**No reports showing up:**
- Check `data/prediction_reports/` exists
- Verify permissions on data directory
- Run a prediction to generate first report

**Import errors:**
- Ensure `src/` is in Python path
- Check `sys.path` includes project root

**Outcome marking not working:**
- Verify `data/agent_performance.db` is writable
- Check database schema is initialized
- Review PerformanceTracker logs

**Quality scores seem wrong:**
- Review weighting factors in `_calculate_data_quality_score()`
- Ensure all agents have valid confidence values
- Check for NaN/None values in consensus metrics

## API Reference

### PredictionReportGenerator

```python
from forecasting.prediction_report import PredictionReportGenerator

generator = PredictionReportGenerator()

report = generator.generate_report(
    question="Your question here",
    domain_classification={
        "primary_domain": "military",
        "confidence": 0.9,
        "secondary_domains": ["geopolitical"]
    },
    agent_responses=[
        {
            "agent_name": "Military Strategy Expert",
            "response": "...",
            "confidence": 0.75,
            "base_weight": 1.0,
            "relevance_boost": 1.5,
            "performance_boost": 1.2,
            "adjusted_weight": 1.8,
            "execution_time_ms": 1000,
            "model": "llama3.1:70b",
            "cached": False
        }
    ],
    consensus_result={
        "strength": 0.8,
        "total_weight": 12.5
    },
    execution_metrics={
        "total_time_ms": 15000,
        "succeeded": 14,
        "failed": 2,
        "cache_hits": 3,
        "circuit_breaker_trips": 0
    }
)

# Access report fields
print(report.data_quality_score)  # 0.0-1.0
print(report.consensus_analysis.agreement_level)  # unanimous/strong/moderate/weak/divergent
print(report.metadata.prediction_id)  # SHA-256 hash

# Save to disk
report_dict = report.to_dict()
with open(f"data/prediction_reports/{report.metadata.prediction_id}.json", 'w') as f:
    json.dump(report_dict, f, indent=2)
```

### PerformanceTracker

```python
from forecasting.performance_tracker import PerformanceTracker

tracker = PerformanceTracker()

# Record prediction
tracker.record_prediction(
    prediction_id="abc123",
    question="Your question",
    question_hash="def456",
    domain="military",
    agent_responses=[...]
)

# Record outcome
tracker.record_outcome(
    prediction_id="abc123",
    outcome="correct",  # or "incorrect" or "partial"
    notes="Prediction verified via news sources"
)

# Get agent metrics
metrics = tracker.get_agent_metrics("Military Strategy Expert")
print(metrics["accuracy"])  # 0.0-1.0
print(metrics["total_predictions"])
print(metrics["domain_performance"])  # {domain: accuracy}

# Get domain specialists
top_military = tracker.get_domain_specialists("military", top_n=3)
for agent_name, accuracy in top_military:
    print(f"{agent_name}: {accuracy:.2%}")
```

## See Also

- [Performance Tracking](./PERFORMANCE_TRACKING.md)
- [Domain Consensus](./DOMAIN_CONSENSUS.md)
- [Resilience Patterns](./RESILIENCE.md)
- [Feed Verification](./FEED_VERIFICATION.md)
